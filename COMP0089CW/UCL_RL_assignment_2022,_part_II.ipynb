{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYs6LMEbNqoQ"
      },
      "source": [
        "# RL coursework, part II (25 pts total)\n",
        "---\n",
        "\n",
        "**SN:** 21180859\n",
        "---\n",
        "\n",
        "**Due date:** *22nd March, 2022,*\n",
        "\n",
        "---\n",
        "\n",
        "Standard UCL policy (including grade deductions) automatically applies for any late submissions.\n",
        "\n",
        "## How to submit\n",
        "\n",
        "When you have completed the exercises and everything has finished running, click on 'File' in the menu-bar and then 'Download .ipynb'. This file must be submitted to Moodle named as **`<studentnumber>_RL_part2.ipynb`** before the deadline above, where `<studentnumber>` is your student number."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9v_SYckYfv5G"
      },
      "source": [
        "## Context\n",
        "\n",
        "In this part, we will take a first look at learning algorithms for sequential decision problems.\n",
        "\n",
        "## Background reading\n",
        "\n",
        "* Sutton and Barto (2018), Chapters 3 - 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNuohp44N00i"
      },
      "source": [
        "# The Assignment\n",
        "\n",
        "### Objectives\n",
        "\n",
        "You will use Python to implement several reinforcement learning algorithms.\n",
        "\n",
        "You will then run these algorithms on a few problems, to understand their properties."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztQEQvnKh2t6"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB0tQ4aiAaIu"
      },
      "source": [
        "### Import Useful Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YzYtxi8Wh5SJ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.collections as mcoll\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NDhSYfSDcCC"
      },
      "source": [
        "### Set options"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ps5OnkPmDbMX"
      },
      "outputs": [],
      "source": [
        "np.set_printoptions(precision=3, suppress=1)\n",
        "plt.style.use('seaborn-colorblind')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALrRR76eAd6u"
      },
      "source": [
        "### Some grid world"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YP97bVN3NuG8"
      },
      "outputs": [],
      "source": [
        "W = -100  # wall\n",
        "G = 100  # goal\n",
        "\n",
        "GRID_LAYOUT = np.array([\n",
        "  [W, W, W, W, W, W, W, W, W, W, W, W],\n",
        "  [W, W, 0, W, W, W, W, W, W, 0, W, W],\n",
        "  [W, 0, 0, 0, 0, 0, 0, 0, 0, G, 0, W],\n",
        "  [W, 0, 0, 0, W, W, W, W, 0, 0, 0, W],\n",
        "  [W, 0, 0, 0, W, W, W, W, 0, 0, 0, W],\n",
        "  [W, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, W],\n",
        "  [W, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, W],\n",
        "  [W, W, 0, 0, 0, 0, 0, 0, 0, 0, W, W],\n",
        "  [W, W, W, W, W, W, W, W, W, W, W, W]\n",
        "])\n",
        "\n",
        "class Grid(object):\n",
        "\n",
        "  def __init__(self, noisy=False):\n",
        "    # -1: wall\n",
        "    # 0: empty, episode continues\n",
        "    # other: number indicates reward, episode will terminate\n",
        "    self._layout = GRID_LAYOUT\n",
        "    self._start_state = (2, 2)\n",
        "    self._state = self._start_state\n",
        "    self._number_of_states = np.prod(np.shape(self._layout))\n",
        "    self._noisy = noisy\n",
        "\n",
        "  @property\n",
        "  def number_of_states(self):\n",
        "    return self._number_of_states\n",
        "\n",
        "  def get_obs(self):\n",
        "    y, x = self._state\n",
        "    return y*self._layout.shape[1] + x\n",
        "\n",
        "  def obs_to_state(self, obs):\n",
        "    x = obs % self._layout.shape[1]\n",
        "    y = obs // self._layout.shape[1]\n",
        "    s = np.copy(grid._layout)\n",
        "    s[y, x] = 4\n",
        "    return s\n",
        "\n",
        "  def step(self, action):\n",
        "    y, x = self._state\n",
        "    \n",
        "    if action == 0:  # up\n",
        "      new_state = (y - 1, x)\n",
        "    elif action == 1:  # right\n",
        "      new_state = (y, x + 1)\n",
        "    elif action == 2:  # down\n",
        "      new_state = (y + 1, x)\n",
        "    elif action == 3:  # left\n",
        "      new_state = (y, x - 1)\n",
        "    else:\n",
        "      raise ValueError(\"Invalid action: {} is not 0, 1, 2, or 3.\".format(action))\n",
        "\n",
        "    new_y, new_x = new_state\n",
        "    reward = self._layout[new_y, new_x]\n",
        "    if self._layout[new_y, new_x] == W:  # wall\n",
        "      discount = 0.9\n",
        "      new_state = (y, x)\n",
        "    elif self._layout[new_y, new_x] == 0:  # empty cell\n",
        "      reward = -1.\n",
        "      discount = 0.9\n",
        "    else:  # a goal\n",
        "      discount = 0.\n",
        "      new_state = self._start_state\n",
        "\n",
        "    if self._noisy:\n",
        "      width = self._layout.shape[1]\n",
        "      reward += 10*np.random.normal(0, width - new_x + new_y)\n",
        "\n",
        "    self._state = new_state\n",
        "    return reward, discount, self.get_obs()\n",
        "\n",
        "  def plot_grid(self):\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.imshow(self._layout != W, interpolation=\"nearest\", cmap='pink')\n",
        "    plt.gca().grid(0)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.title(\"The grid\")\n",
        "    plt.text(2, 2, r\"$\\mathbf{S}$\", ha='center', va='center')\n",
        "    plt.text(9, 2, r\"$\\mathbf{G}$\", ha='center', va='center')\n",
        "    h, w = self._layout.shape\n",
        "    for y in range(h-1):\n",
        "      plt.plot([-0.5, w-0.5], [y+0.5, y+0.5], '-k', lw=2)\n",
        "    for x in range(w-1):\n",
        "      plt.plot([x+0.5, x+0.5], [-0.5, h-0.5], '-k', lw=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOu9RZY3AkF1"
      },
      "source": [
        "### Helper functions\n",
        "(You should not have to change, or even look at, these.  Do run the cell to make sure the functions are loaded though.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EttQGJ1n5Zn"
      },
      "outputs": [],
      "source": [
        "def run_experiment(env, agent, number_of_steps):\n",
        "  mean_reward = 0.\n",
        "  try:\n",
        "    action = agent.initial_action()\n",
        "  except AttributeError:\n",
        "    action = 0\n",
        "  for _ in range(number_of_steps):\n",
        "    reward, discount, next_state = env.step(action)\n",
        "    action = agent.step(reward, discount, next_state)\n",
        "    mean_reward += reward\n",
        "  return mean_reward/float(number_of_steps)\n",
        "\n",
        "map_from_action_to_subplot = lambda a: (2, 6, 8, 4)[a]\n",
        "map_from_action_to_name = lambda a: (\"up\", \"right\", \"down\", \"left\")[a]\n",
        "\n",
        "def plot_values(grid, values, colormap='pink', vmin=0, vmax=10):\n",
        "  plt.imshow(values - 1000*(grid<0), interpolation=\"nearest\", cmap=colormap, vmin=vmin, vmax=vmax)\n",
        "  plt.yticks([])\n",
        "  plt.xticks([])\n",
        "  plt.colorbar(ticks=[vmin, vmax])\n",
        "\n",
        "def plot_action_values(grid, action_values, vmin=-5, vmax=5):\n",
        "  q = action_values\n",
        "  fig = plt.figure(figsize=(10, 10))\n",
        "  fig.subplots_adjust(wspace=0.3, hspace=0.3)\n",
        "  for a in [0, 1, 2, 3]:\n",
        "    plt.subplot(4, 3, map_from_action_to_subplot(a))\n",
        "    plot_values(grid, q[..., a], vmin=vmin, vmax=vmax)\n",
        "    action_name = map_from_action_to_name(a)\n",
        "    plt.title(r\"$q(s, \\mathrm{\" + action_name + r\"})$\")\n",
        "    \n",
        "  plt.subplot(4, 3, 5)\n",
        "  v = np.max(q, axis=-1)\n",
        "  plot_values(grid, v, colormap='summer', vmin=vmin, vmax=vmax)\n",
        "  plt.title(\"$v(s)$\")\n",
        "  \n",
        "  # Plot arrows:\n",
        "  plt.subplot(4, 3, 11)\n",
        "  plot_values(grid, grid==0, vmax=1)\n",
        "  for row in range(len(grid)):\n",
        "    for col in range(len(grid[0])):\n",
        "      if grid[row][col] == 0:\n",
        "        argmax_a = np.argmax(q[row, col])\n",
        "        if argmax_a == 0:\n",
        "          x = col\n",
        "          y = row + 0.5\n",
        "          dx = 0\n",
        "          dy = -0.8\n",
        "        if argmax_a == 1:\n",
        "          x = col - 0.5\n",
        "          y = row\n",
        "          dx = 0.8\n",
        "          dy = 0\n",
        "        if argmax_a == 2:\n",
        "          x = col\n",
        "          y = row - 0.5\n",
        "          dx = 0\n",
        "          dy = 0.8\n",
        "        if argmax_a == 3:\n",
        "          x = col + 0.5\n",
        "          y = row\n",
        "          dx = -0.8\n",
        "          dy = 0\n",
        "        plt.arrow(x, y, dx, dy, width=0.02, head_width=0.4, head_length=0.4, length_includes_head=True, fc='k', ec='k')\n",
        "\n",
        "def plot_rewards(xs, rewards, color):\n",
        "  mean = np.mean(rewards, axis=0)\n",
        "  p90 = np.percentile(rewards, 90, axis=0)\n",
        "  p10 = np.percentile(rewards, 10, axis=0)\n",
        "  plt.plot(xs, mean, color=color, alpha=0.6)\n",
        "  plt.fill_between(xs, p90, p10, color=color, alpha=0.3)\n",
        "\n",
        "def parameter_study(parameter_values, parameter_name, agent_constructor,\n",
        "                    env_constructor, color,\n",
        "                    repetitions=10, number_of_steps=int(1e4)):\n",
        "  mean_rewards = np.zeros((repetitions, len(parameter_values)))\n",
        "  greedy_rewards = np.zeros((repetitions, len(parameter_values)))\n",
        "  for rep in range(repetitions):\n",
        "    for i, p in enumerate(parameter_values):\n",
        "      env = env_constructor()\n",
        "      agent = agent_constructor()\n",
        "      if 'eps' in parameter_name:\n",
        "        agent.set_epsilon(p)\n",
        "      elif 'alpha' in parameter_name:\n",
        "        agent._step_size = p\n",
        "      else:\n",
        "        raise NameError(\"Unknown parameter_name: {}\".format(parameter_name))\n",
        "      mean_rewards[rep, i] = run_experiment(env, agent, number_of_steps)\n",
        "      agent.set_epsilon(0.)\n",
        "      agent._step_size = 0.\n",
        "      greedy_rewards[rep, i] = run_experiment(env, agent, number_of_steps//10)\n",
        "      del env\n",
        "      del agent\n",
        "\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plot_rewards(parameter_values, mean_rewards, color)\n",
        "  plt.yticks=([0, 1], [0, 1])\n",
        "  plt.ylabel(\"Average reward over first {} steps\".format(number_of_steps), size=12)\n",
        "  plt.xlabel(parameter_name, size=12)\n",
        "\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plot_rewards(parameter_values, greedy_rewards, color)\n",
        "  plt.yticks=([0, 1], [0, 1])\n",
        "  plt.ylabel(\"Final rewards, with greedy policy\".format(number_of_steps), size=12)\n",
        "  plt.xlabel(parameter_name, size=12)\n",
        "\n",
        "def epsilon_greedy(q_values, epsilon):\n",
        "  if epsilon < np.random.random():\n",
        "    return np.argmax(q_values)\n",
        "  else:\n",
        "    return np.random.randint(np.array(q_values).shape[-1])\n",
        "\n",
        "\n",
        "def colorline(x, y, z):\n",
        "  \"\"\"\n",
        "  Based on:\n",
        "  http://nbviewer.ipython.org/github/dpsanders/matplotlib-examples/blob/master/colorline.ipynb\n",
        "  http://matplotlib.org/examples/pylab_examples/multicolored_line.html\n",
        "  Plot a colored line with coordinates x and y\n",
        "  Optionally specify colors in the array z\n",
        "  Optionally specify a colormap, a norm function and a line width\n",
        "  \"\"\"\n",
        "  segments = make_segments(x, y)\n",
        "  lc = mcoll.LineCollection(segments, array=z, cmap=plt.get_cmap('copper_r'),\n",
        "                            norm=plt.Normalize(0.0, 1.0), linewidth=3)\n",
        "\n",
        "  ax = plt.gca()\n",
        "  ax.add_collection(lc)\n",
        "  return lc\n",
        "\n",
        "\n",
        "def make_segments(x, y):\n",
        "  \"\"\"\n",
        "  Create list of line segments from x and y coordinates, in the correct format\n",
        "  for LineCollection: an array of the form numlines x (points per line) x 2 (x\n",
        "  and y) array\n",
        "  \"\"\"\n",
        "  points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
        "  segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
        "  return segments\n",
        "\n",
        "\n",
        "def plotting_helper_function(_x, _y, title=None, ylabel=None):\n",
        "  z = np.linspace(0, 0.9, len(_x))**0.7\n",
        "  colorline(_x, _y, z)\n",
        "  plt.plot(0, 0, '*', color='#000000', ms=20, alpha=0.7, label='$w^*$')\n",
        "  plt.plot(1, 1, '.', color='#ee0000', alpha=0.7, ms=20, label='$w_0$')\n",
        "  min_y, max_y = np.min(_y), np.max(_y)\n",
        "  min_x, max_x = np.min(_x), np.max(_x)\n",
        "  min_y, max_y = np.min([0, min_y]), np.max([0, max_y])\n",
        "  min_x, max_x = np.min([0, min_x]), np.max([0, max_x])\n",
        "  range_y = max_y - min_y\n",
        "  range_x = max_x - min_x\n",
        "  max_range = np.max([range_y, range_x])\n",
        "  plt.arrow(_x[-3], _y[-3], _x[-1] - _x[-3], _y[-1] - _y[-3], color='k',\n",
        "            head_width=0.04*max_range, head_length=0.04*max_range,\n",
        "            head_starts_at_zero=False)\n",
        "  plt.ylim(min_y - 0.2*range_y, max_y + 0.2*range_y)\n",
        "  plt.xlim(min_x - 0.2*range_x, max_x + 0.2*range_x)\n",
        "  ax = plt.gca()\n",
        "  ax.ticklabel_format(style='plain', useMathText=True)\n",
        "  plt.legend(loc=2)\n",
        "  plt.xticks(rotation=12, fontsize=10)\n",
        "  plt.yticks(rotation=12, fontsize=10)\n",
        "  plt.locator_params(nbins=3)\n",
        "  if title is not None:\n",
        "    plt.title(title, fontsize=20)\n",
        "  if ylabel is not None:\n",
        "    plt.ylabel(ylabel, fontsize=20)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzpb_dGVjT0O"
      },
      "source": [
        "# Section 1: Tabular RL\n",
        "\n",
        "In this section, observations will be states in the environment, so the agent state, environment state, and observation will all be the same, and we will use the word `state` interchangably with `observation`.  You will implement agents, which should be in pure Python - so you cannot use JAX/TensorFlow/PyTorch to compute gradients. Using `numpy` is fine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0Z5IgXfU2Qw"
      },
      "source": [
        "### A random agent\n",
        "\n",
        "Below we show a reference implementation of a simple random agent, implemented according to the interface above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mf64o3b3U6A4"
      },
      "outputs": [],
      "source": [
        "class Random(object):\n",
        "\n",
        "  def __init__(self, number_of_actions, number_of_states, initial_state):\n",
        "    self._number_of_actions = number_of_actions\n",
        "\n",
        "  def step(self, reward, discount, next_state):\n",
        "    next_action = np.random.randint(self._number_of_actions)\n",
        "    return next_action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaGeLcsvixmt"
      },
      "source": [
        "### The grid\n",
        "\n",
        "The cell below shows the `Grid` environment that we will use in this section. Here `S` indicates the start state and `G` indicates the goal.  The agent has four possible actions: up, right, down, and left.  Rewards are: `-100` for bumping into a wall, `+100` for reaching the goal, and `-1` otherwise.  The episode ends when the agent reaches the goal, and otherwise continues.  The discount, on continuing steps, is $\\gamma = 0.9$.  Feel free to reference the implemetation of the `Grid` above, under the header \"a grid world\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlFuWFzIi5uB"
      },
      "outputs": [],
      "source": [
        "GRID = Grid()\n",
        "GRID.plot_grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8oKd0oyvNcH"
      },
      "source": [
        "\n",
        "## Q1: Implement TD learning **[4 pts]**\n",
        "Implement an agent that acts randomly, and _on-policy_ estimates state values $v(s)$, using one-step TD learning with step size $\\alpha=0.1$.\n",
        "\n",
        "Use the `__init__` as provided below.  You should implement the `step` function.  We store the initial state in the constructor because you need its value on the first `step` in order to compute the TD error when the first transition has occurred. \n",
        "\n",
        "Also implement a property `state_values(self)` returning the vector of all state values (one value per state). (A method with the `@property` decorator can be called without the parentheses as if it's a variable, e.g., `agent.state_values` instead of `agent.state_values()`.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yKt1qYYjWTR"
      },
      "outputs": [],
      "source": [
        "class RandomTD(object):\n",
        "\n",
        "  def __init__(self, number_of_states, number_of_actions, initial_state, step_size=0.1):\n",
        "    self.state = initial_state\n",
        "    self.number_of_states = number_of_states \n",
        "    self.number_of_actions = number_of_actions\n",
        "    self.step_size = 0.1\n",
        "    self.values  = np.zeros(number_of_states)\n",
        "  @property\n",
        "  def state_values(self):\n",
        "    return self.values\n",
        "\n",
        "  def step(self, reward, discount, next_state):\n",
        "    \n",
        "    targ = reward + discount * self.values[next_state]\n",
        "    err = targ - self.values[self.state]\n",
        "    self.values[self.state] += self.step_size * err\n",
        "    self.state = next_state\n",
        "    next_action = np.random.randint(self.number_of_actions)\n",
        "    return next_action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaMmp1lDgpUG"
      },
      "source": [
        "### Run the next cell to run the `RandomTD` agent on a grid world.\n",
        "\n",
        "If everything worked as expected, the plot below will show the estimates state values under the random policy. This includes values for unreachable states --- on the walls and on the goal (we never actually reach the goal --- rather, the episode terminates on the transition to the goal.  The values on the walls and goal are, and will always remain, zero. The plotting code knows about the walls, so they appear black below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0ZoYwgZfho2"
      },
      "outputs": [],
      "source": [
        "# Do not modify this cell.\n",
        "AGENT = RandomTD(GRID._layout.size, 4, GRID.get_obs())\n",
        "run_experiment(GRID, AGENT, int(1e5))\n",
        "v = AGENT.state_values\n",
        "plot_values(GRID_LAYOUT,\n",
        "            v.reshape(GRID._layout.shape),\n",
        "            colormap=\"pink\", vmin=-400, vmax=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxc_Sx7og4JH"
      },
      "source": [
        "## Q2: Policy iteration **[3 pts]**\n",
        "We used TD to do policy evaluation for the random policy on this problem.  Consider doing policy improvement, by taking the greedy policy with respect to a one-step look-ahead.  For this, you may assume we have a true model, so for each state and for each action we can look at the value of the resulting state, and would then pick the action with the highest reward plus subsequent state value. In other words, you can assume we can use $q(s, a) = \\mathbb{E}[ R_{t+1} + \\gamma v(S_{t+1}) \\mid S_t = s, A_t = a]$, where $v$ is the value function learned by TD as implemented. Then we consider the policy that picks the action with the highest action value $q(s, a)$. You do **not** have to implement this, just answer the following question.\n",
        "\n",
        "The above amounts to performing an iteration of policy evaluation and policy improvement.  If we repeat this process over and over again, and repeatedly evaluate the greedy policy and then perform an improvement step by picking the greedy policy, would the policy eventually become optimal?  Explain why or why not in at most three sentences.\n",
        "\n",
        "> Since our TD algorithm picked actions randomly, it estimated that picking states which go through the upper part of the grid is worse than picking those which go through the lower part of the grid because clearly the upper route is riskier due to a low margin of error (one step up or down and we bump into a wall). Hence, by doing policy improvement via the greedy policy, we are going to end up with a policy which goes via the lower part of the grid, and hence the policy would **not** be optimal (the upper route takes 7 steps and the lower route takes at least 13)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4R58NP87jbM"
      },
      "source": [
        "# Section 2: Off-policy Bellman operators with function approximation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIyALFeoiov9"
      },
      "source": [
        "## Q3: Bellman operator for prediction **[5 pts]**\n",
        "\n",
        "We are going to implement the _expected_ update to the weight for a simple MDP. \n",
        "In order to be able to implement this update, we need to know the MDP, as well as the way the agent's value depend on the weights and states.  This is defined as follows:\n",
        "\n",
        "There are two states, $s_1$ and $s_2$.  All rewards are zero, and therefore can be ignored.  Instead of the underlying state, the agent observes state features $\\mathbf{x}_1 = \\mathbf{x}(s_1)$ and $\\mathbf{x}_2 = \\mathbf{x}(s_2)$.  These are defined for the two states are $\\mathbf{x}_1 = [1, 1]^{\\top}$ and $\\mathbf{x}_2 = [2, 1]^{\\top}$.  In each state, there are two actions, $a$ and $b$.  Action $a$ always transitions to state $s_1$, action $b$ always transitions to state $s_2$.\n",
        "\n",
        "![MDP](https://hadovanhasselt.files.wordpress.com/2020/02/mdp.png)\n",
        "\n",
        "---\n",
        "\n",
        "The agent will make *linear* predicions, such that $v(s) = \\mathbf{w}^\\top \\mathbf{x}(s)$. In the code cell below, you should implement an update that computes the **expected** weight update, given:\n",
        "* The current weights `w`.\n",
        "* A target policy $\\pi$ that we want to learn about; the actual input, denoted `pi` in the code below, will be a scalar indicating the probability of selecting action `a` in both states: `pi` $= \\pi(a|s), \\forall s$.\n",
        "* A behaviour policy $\\mu$ that would be used to generate the transitions; the actual input, denoted `mu` in the code below, will be a scalar indicating the probability of selecting action `a` in both states: `mu` $= \\mu(a|s), \\forall s$.\n",
        "* A scalar trace parameter $\\lambda$ (=`trace_parameter`),\n",
        "* A scalar discount parameter $\\gamma$ (=`discount`).\n",
        "\n",
        "The expectation should take into account the probabilities of actions in the future, as well as the steady-state (=long-term) probability of being in a state.  The step size of the update should be $\\alpha=0.1$.\n",
        "\n",
        "The expected update should be for a multi-step $\\lambda$-return and should be correct for any pair of target and any behaviour policy.  It should be the expectation of performing a (forward view) TD($\\lambda$) update in the MDP described above when the _state distribution_ is generated by the behaviour policy and the _returns_ from each state are generated by the target policy. We will use the update you implement to generate plots below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEpxlyUtrj3i"
      },
      "outputs": [],
      "source": [
        "# state features (do not change)\n",
        "x1 = np.array([1., 1.])\n",
        "x2 = np.array([2., 1.])\n",
        "\n",
        "def expected_update(w, pi, mu, trace_parameter, discount, lr=0.1):\n",
        "  G_v = (1-trace_parameter)*discount*(w.dot(x1)*pi + (1-pi)*w.dot(x2))/(1-discount*trace_parameter)\n",
        "  expected_weight_update = lr*(mu*(G_v - w.dot(x1))*x1 + (1-mu)*(G_v - w.dot(x2))*x2)\n",
        "\n",
        "  return expected_weight_update\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U42IsCPW0KGY"
      },
      "source": [
        "##Experiment 3: run the cell below\n",
        "The cell below runs an experiment, across different target policies and trace parameters $\\lambda$.\n",
        "\n",
        "The plots below the cell will show how the weights move within the 2-dimensional weight space, starting from $w_0 = [1, 1]^{\\top}$ (shown as red dot).  The optimal solution $w_* = [0, 0]^{\\top}$ is also shown (as black star)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTFFpQSX0Eaj"
      },
      "outputs": [],
      "source": [
        "def generate_ws(w, pi, mu, l, g):\n",
        "  \"\"\"Apply the expected update 1000 times\"\"\"\n",
        "  ws = [w]\n",
        "  for _ in range(1000):\n",
        "    w = w + expected_update(w, pi, mu, l, g, lr=0.1)\n",
        "    ws.append(w)\n",
        "  return np.array(ws)\n",
        "\n",
        "mu = 0.5  # behaviour\n",
        "g = 0.99  # discount\n",
        "\n",
        "lambdas = np.array([0, 0.8, 0.9, 0.95, 1.])\n",
        "pis = np.array([0., 0.1, 0.2, 0.5, 1.])\n",
        "\n",
        "fig = plt.figure(figsize=(22, 17))\n",
        "fig.subplots_adjust(wspace=0.25, hspace=0.3)\n",
        "\n",
        "for r, pi in enumerate(pis):\n",
        "  for c, l in enumerate(lambdas):\n",
        "    plt.subplot(len(pis), len(lambdas), r*len(lambdas) + c + 1)\n",
        "    w = np.ones_like(x1)\n",
        "    ws = generate_ws(w, pi, mu, l, g)\n",
        "    title = '$\\\\lambda={:1.3f}$'.format(l) if r == 0 else None\n",
        "    ylabel = '$\\\\pi={:1.1f}$'.format(pi) if c == 0 else None\n",
        "    plotting_helper_function(ws[:, 0], ws[:, 1], title, ylabel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KxL4o357_dt"
      },
      "source": [
        "## Q4: Analyse results (13 pts total)\n",
        "1. **[1 pts]** How many of the above 25 experiments diverge?\n",
        "1. **[1 pts]** For which policies $\\pi$, is the true value function $v_{\\pi}$ representable in the above feature space (spanned by $x_1, x_2$).\n",
        "1. **[2 pts]** Why are the results asymmetric across different $\\pi$?  In particular, explain why the results look different when comparing $\\pi = \\pi(a | \\cdot) = 0$ to $\\pi(a | \\cdot) = 1$.\n",
        "1. **[2 pts]** For which combination of $\\pi(a)$ and $\\lambda$ does the expected update (with uniform random behaviour) converge? Do not limit the answer to the subset of values in the plots above, but to all possible choices of $\\lambda \\in [0, 1]$ and $\\pi(a|s) \\in [0, 1]$, but do restrict yourself to state-less policies, as above, for which the action probabilities are equal in the two states: $\\pi(a|s_1) = \\pi(a|s_2)$.\n",
        "1. **[1 pts]** Why do all the plots corresponding to full Monte Carlo update look the same (right column)?\n",
        "1. **[2 pts]** Why do the plots corresponding to full Monte Carlo have the shape they do?\n",
        "1. **[2 pts]** The plots above are for uniform behavoiur: $\\mu(a|s) = \\mu(b|s) = 0.5$.  How would the results change (high level, not in terms of precise plots) if the behaviour policy $\\mu$ would select action $a$ more often (e.g., $\\mu = 0.8$)?  How would the results change if the behaviour would select $a$ less often (e.g., $\\mu = 0.2$)?\n",
        "1. **[2 pts]** Consider again the orginal experiment, where data is gathered under uniformly random behaviour policy. What would the updates to the vectors $w$ be under the $L_\\infty$ norm? You can either run the experiment or give the closed-form update in an equation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0pm2-4rvB49"
      },
      "source": [
        "## Put answers to Q4 in this cell:\n",
        "1. 7 diverge.\n",
        "2. For all $\\pi$ with the condition that $w^*=(0,0)$.\n",
        "3.  We have that when $\\pi(a | \\cdot)=0$, the algorithm bootstraps only on $x_1$ and has an expected negative update (on average) and hence we expect the weights to converge to the optimal solution $w^*=(0,0)$. On the other hand  when $\\pi(a | \\cdot)=0$, the algorithm bootstraps only on $x_2$ and has an expected positive update for nearly all $\\lambda$. Unless $\\lambda$ is close to $1$, the weights will diverge because of this. \n",
        "4. The following inequality shows for which combinations of of $\\pi(a)$ and $\\lambda$ the expected update converges:\n",
        "\n",
        "$$\\left(\\frac{\\gamma(1-\\lambda)(\\pi x_1 + (1-\\pi)x_2)}{1-\\gamma \\lambda}-x_1\\right)\\cdot x_1 + \\left(\\frac{\\gamma(1-\\lambda)(\\pi x_1 + (1-\\pi)x_2)}{1-\\gamma \\lambda}-x_2\\right)\\cdot x_2 < 0$$\n",
        "5. Because in Monte Carlo, $\\lambda = 1$, so $\\pi$ does not influence the expected weight update and hence they are the same for all $\\pi$\n",
        "6. We have that the expected weight update is $$w_{t+1}=w_t-a \\left(w_t\\cdot x_1)x_1)+(w_t\\cdot x_2)x_2\\right)$$ for some $a\\in \\mathbb{R}$. From this formula we can see that when the state features for $x_2$ are greated in the first coordinate of $w$, the first coordinate decreases faster than the second. After decreasing, we can see that the the update for the first weight coordinate becomes positive and negative for the second coordinate, causing them to cancel out and hence converging to the optimal solution $(0,0)$\n",
        "7. With $\\mu=0.8$, we would have more divergent experiments the update term of feature $x_1$ would increase a lot, resulting in bigger updates which would cause divergence. The opposite can be said for when $\\mu=0.2$ - there would be more convergent experiments.\n",
        "8. _...answer here..._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhP_7uHmsaUw"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "UCL_RL_assignment_2022,_part_II.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}