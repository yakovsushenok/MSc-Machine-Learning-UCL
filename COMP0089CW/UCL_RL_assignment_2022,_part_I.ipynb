{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYs6LMEbNqoQ"
      },
      "source": [
        "# RL coursework, part I (20 pts total)\n",
        "---\n",
        "\n",
        "**SN:** 21180859\n",
        "\n",
        "---\n",
        "\n",
        "**Due date:** *22nd March, 2022,*\n",
        "\n",
        "---\n",
        "\n",
        "Standard UCL policy (including grade deductions) automatically applies for any late submissions.\n",
        "\n",
        "## How to submit\n",
        "\n",
        "When you have completed the exercises and everything has finished running, click on 'File' in the menu-bar and then 'Download .ipynb'. This file must be submitted to Moodle named as **`<studentnumber>_RL_part1.ipynb`** before the deadline above, where `<studentnumber>` is your student number."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9v_SYckYfv5G"
      },
      "source": [
        "**Context**\n",
        "\n",
        "In this assignment, we will take a first look at learning decisions from data.  For this, we will use the multi-armed bandit framework.\n",
        "\n",
        "**Background reading**\n",
        "\n",
        "* Sutton and Barto (2018), Chapters 1 to 6\n",
        "* Lecture slides"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNuohp44N00i"
      },
      "source": [
        "**Overview of this assignment**\n",
        "\n",
        "A) You will use Python to implement several bandit algorithms.\n",
        "\n",
        "B) You will then run these algorithms on a multi-armed Bernoulli bandit problem, and answer question about their empirical performance.\n",
        "\n",
        "C) You will then be asked to reason about the behaviour of different algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztQEQvnKh2t6"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Run each of the cells below, until you reach the next section **Basic Agents**. You do not have to read or understand the code in the **Setup** section.  After running the cells, feel free to fold away the **Setup** section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YzYtxi8Wh5SJ"
      },
      "outputs": [],
      "source": [
        "# Import Useful Libraries\n",
        "\n",
        "import collections\n",
        "from functools import partial\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "np.set_printoptions(precision=3, suppress=1)\n",
        "plt.style.use('seaborn-notebook')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YP97bVN3NuG8"
      },
      "outputs": [],
      "source": [
        "class BernoulliBandit(object):\n",
        "  \"\"\"A stationary multi-armed Bernoulli bandit.\"\"\"\n",
        "\n",
        "  def __init__(self, success_probabilities, success_reward=1., fail_reward=0.):\n",
        "    \"\"\"Constructor of a stationary Bernoulli bandit.\n",
        "\n",
        "    Args:\n",
        "      success_probabilities: A list or numpy array containing the probabilities,\n",
        "          for each of the arms, of providing a success reward.\n",
        "      success_reward: The reward on success (default: 1.)\n",
        "      fail_reward: The reward on failure (default: 0.)\n",
        "    \"\"\"\n",
        "    self._probs = success_probabilities\n",
        "    self._number_of_arms = len(self._probs)\n",
        "    self._s = success_reward\n",
        "    self._f = fail_reward\n",
        "\n",
        "    ps = np.array(success_probabilities)\n",
        "    self._values = ps * success_reward + (1 - ps) * fail_reward\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"The step function.\n",
        "\n",
        "    Args:\n",
        "      action: An integer or np.int32 that specifies which arm to pull.\n",
        "\n",
        "    Returns:\n",
        "      A reward sampled according to the success probability of the selected arm.\n",
        "\n",
        "    Raises:\n",
        "      ValueError: when the provided action is out of bounds.\n",
        "    \"\"\"\n",
        "    if action < 0 or action >= self._number_of_arms:\n",
        "      raise ValueError('Action {} is out of bounds for a '\n",
        "                       '{}-armed bandit'.format(action, self._number_of_arms))\n",
        "\n",
        "    success = bool(np.random.random() < self._probs[action])\n",
        "    reward = success * self._s + (not success) * self._f\n",
        "    return reward\n",
        "\n",
        "  def regret(self, action):\n",
        "    \"\"\"Computes the regret for the given action.\"\"\"\n",
        "    return self._values.max() - self._values[action]\n",
        "\n",
        "  def optimal_value(self):\n",
        "    \"\"\"Computes the regret for the given action.\"\"\"\n",
        "    return self._values.max()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYxNiGcRxbd0"
      },
      "outputs": [],
      "source": [
        "class NonStationaryBandit(object):\n",
        "  \"\"\"A non-stationary multi-armed Bernoulli bandit.\"\"\"\n",
        "\n",
        "  def __init__(self, success_probabilities,\n",
        "               success_reward=1., fail_reward=0., change_point=800,\n",
        "               change_is_good=True):\n",
        "    \"\"\"Constructor of a non-stationary Bernoulli bandit.\n",
        "\n",
        "    Args:\n",
        "      success_probabilities: A list or numpy array containing the probabilities,\n",
        "          for each of the arms, of providing a success reward.\n",
        "      success_reward: The reward on success (default: 1.)\n",
        "      fail_reward: The reward on failure (default: 0.)\n",
        "      change_point: The number of steps before the rewards change.\n",
        "      change_is_good: Whether the rewards go up (if True), or flip (if False).\n",
        "    \"\"\"\n",
        "    self._probs = success_probabilities\n",
        "    self._number_of_arms = len(self._probs)\n",
        "    self._s = success_reward\n",
        "    self._f = fail_reward\n",
        "    self._change_point = change_point\n",
        "    self._change_is_good = change_is_good\n",
        "    self._number_of_steps_so_far = 0\n",
        "\n",
        "    ps = np.array(success_probabilities)\n",
        "    self._values = ps * success_reward + (1 - ps) * fail_reward\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"The step function.\n",
        "\n",
        "    Args:\n",
        "      action: An integer or np.int32 that specifies which arm to pull.\n",
        "\n",
        "    Returns:\n",
        "      A reward sampled according to the success probability of the selected arm.\n",
        "\n",
        "    Raises:\n",
        "      ValueError: when the provided action is out of bounds.\n",
        "    \"\"\"\n",
        "    if action < 0 or action >= self._number_of_arms:\n",
        "      raise ValueError('Action {} is out of bounds for a '\n",
        "                       '{}-armed bandit'.format(action, self._number_of_arms))\n",
        "\n",
        "    self._number_of_steps_so_far += 1\n",
        "    success = bool(np.random.random() < self._probs[action])\n",
        "    reward = success * self._s + (not success) * self._f\n",
        "    \n",
        "    if self._number_of_steps_so_far == self._change_point:\n",
        "      # After some number of steps, the rewards are inverted\n",
        "      #\n",
        "      #  ``The past was alterable. The past never had been altered. Oceania was\n",
        "      #    at war with Eastasia. Oceania had always been at war with Eastasia.``\n",
        "      #            - 1984, Orwell (1949).\n",
        "      reward_dif = (self._s - self._f)\n",
        "      if self._change_is_good:\n",
        "        self._f = self._s + reward_dif\n",
        "      else:\n",
        "        self._s -= reward_dif\n",
        "        self._f += reward_dif\n",
        "      \n",
        "      # Recompute expected values when the rewards change\n",
        "      ps = np.array(self._probs)\n",
        "      self._values = ps * self._s + (1 - ps) * self._f\n",
        "\n",
        "    return reward\n",
        "  \n",
        "  def regret(self, action):\n",
        "    \"\"\"Computes the regret for the given action.\"\"\"\n",
        "    return self._values.max() - self._values[action]\n",
        "  \n",
        "  def optimal_value(self):\n",
        "    \"\"\"Computes the regret for the given action.\"\"\"\n",
        "    return self._values.max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DU7KGFJ0DN-H"
      },
      "outputs": [],
      "source": [
        "# Helper functions\n",
        "\n",
        "def smooth(array, smoothing_horizon=100., initial_value=0.):\n",
        "  \"\"\"Smoothing function for plotting.\"\"\"\n",
        "  smoothed_array = []\n",
        "  value = initial_value\n",
        "  b = 1./smoothing_horizon\n",
        "  m = 1.\n",
        "  for x in array:\n",
        "    m *= 1. - b\n",
        "    lr = b/(1 - m)\n",
        "    value += lr*(x - value)\n",
        "    smoothed_array.append(value)\n",
        "  return np.array(smoothed_array)\n",
        "\n",
        "def plot(algs, plot_data, repetitions=30):\n",
        "  \"\"\"Plot results of a bandit experiment.\"\"\"\n",
        "  algs_per_row = 4\n",
        "  n_algs = len(algs)\n",
        "  n_rows = (n_algs - 2)//algs_per_row + 1\n",
        "  fig = plt.figure(figsize=(10, 4*n_rows))\n",
        "  fig.subplots_adjust(wspace=0.3, hspace=0.35)\n",
        "  clrs = ['#000000', '#00bb88', '#0033ff', '#aa3399', '#ff6600']\n",
        "  lss = ['--', '-', '-', '-', '-']\n",
        "  for i, p in enumerate(plot_data):\n",
        "    for c in range(n_rows):\n",
        "      ax = fig.add_subplot(n_rows, len(plot_data), i + 1 + c*len(plot_data))\n",
        "      ax.grid(0)\n",
        "\n",
        "      current_algs = [algs[0]] + algs[c*algs_per_row + 1:(c + 1)*algs_per_row + 1]\n",
        "      for alg, clr, ls in zip(current_algs, clrs, lss):\n",
        "        data = p.data[alg.name]\n",
        "        m = smooth(np.mean(data, axis=0))\n",
        "        s = np.std(smooth(data.T).T, axis=0)/np.sqrt(repetitions)\n",
        "        if p.log_plot:\n",
        "          line = plt.semilogy(m, alpha=0.7, label=alg.name,\n",
        "                              color=clr, ls=ls, lw=3)[0]\n",
        "        else:\n",
        "          line = plt.plot(m, alpha=0.7, label=alg.name,\n",
        "                          color=clr, ls=ls, lw=3)[0]\n",
        "          plt.fill_between(range(len(m)), m + s, m - s,\n",
        "                           color=line.get_color(), alpha=0.2)\n",
        "      if p.opt_values is not None:\n",
        "        plt.plot(p.opt_values[current_algs[0].name][0], ':', alpha=0.5,\n",
        "                 label='optimal')\n",
        "\n",
        "      ax.set_facecolor('white')\n",
        "      ax.tick_params(axis=\"both\", which=\"both\", bottom=\"off\", top=\"off\",\n",
        "                     labelbottom=\"on\", left=\"off\", right=\"off\", labelleft=\"on\")\n",
        "      ax.spines[\"top\"].set_visible(False)\n",
        "      ax.spines[\"bottom\"].set(visible=True, color='black', lw=1)\n",
        "      ax.spines[\"right\"].set_visible(False)\n",
        "      ax.spines[\"left\"].set(visible=True, color='black', lw=1)\n",
        "      ax.get_xaxis().tick_bottom()\n",
        "      ax.get_yaxis().tick_left()\n",
        "\n",
        "      data = np.array([smooth(np.mean(d, axis=0)) for d in p.data.values()])\n",
        "      \n",
        "      if p.log_plot:\n",
        "        start, end = calculate_lims(data, p.log_plot)\n",
        "        start = np.floor(np.log10(start))\n",
        "        end = np.ceil(np.log10(end))\n",
        "        ticks = [_*10**__\n",
        "                 for _ in [1., 2., 3., 5.]\n",
        "                 for __ in [-2., -1., 0.]]\n",
        "        labels = [r'${:1.2f}$'.format(_*10** __)\n",
        "                  for _ in [1, 2, 3, 5]\n",
        "                  for __ in [-2, -1, 0]]\n",
        "        plt.yticks(ticks, labels)\n",
        "      plt.ylim(calculate_lims(data, p.log_plot))\n",
        "      plt.locator_params(axis='x', nbins=4)\n",
        "      \n",
        "      plt.title(p.title)\n",
        "      if i == len(plot_data) - 1:\n",
        "        plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
        "\n",
        "def run_experiment(bandit_constructor, algs, repetitions, number_of_steps):\n",
        "  \"\"\"Run multiple repetitions of a bandit experiment.\"\"\"\n",
        "  reward_dict = {}\n",
        "  regret_dict = {}\n",
        "  optimal_value_dict = {}\n",
        "\n",
        "  for alg in algs:\n",
        "    reward_dict[alg.name] = np.zeros((repetitions, number_of_steps))\n",
        "    regret_dict[alg.name] = np.zeros((repetitions, number_of_steps))\n",
        "    optimal_value_dict[alg.name] = np.zeros((repetitions, number_of_steps))\n",
        "\n",
        "    for _rep in range(repetitions):\n",
        "      bandit = bandit_constructor()\n",
        "      alg.reset()\n",
        "\n",
        "      action = None\n",
        "      reward = None\n",
        "      for _step in range(number_of_steps):\n",
        "        action = alg.step(action, reward)\n",
        "        reward = bandit.step(action)\n",
        "        regret = bandit.regret(action)\n",
        "        optimal_value = bandit.optimal_value()\n",
        "\n",
        "        reward_dict[alg.name][_rep, _step] = reward\n",
        "        regret_dict[alg.name][_rep, _step] = regret\n",
        "        optimal_value_dict[alg.name][_rep, _step] = optimal_value\n",
        "\n",
        "  return reward_dict, regret_dict, optimal_value_dict\n",
        "\n",
        "\n",
        "def train_agents(agents, number_of_arms, number_of_steps, repetitions=100,\n",
        "                 success_reward=1., fail_reward=0.,\n",
        "                 bandit_class=BernoulliBandit):\n",
        "\n",
        "  success_probabilities = np.arange(0.3, 0.7 + 1e-6, 0.4/(number_of_arms - 1))\n",
        "\n",
        "  bandit_constructor = partial(bandit_class,\n",
        "                               success_probabilities=success_probabilities,\n",
        "                               success_reward=success_reward,\n",
        "                               fail_reward=fail_reward)\n",
        "  rewards, regrets, opt_values = run_experiment(\n",
        "      bandit_constructor, agents, repetitions, number_of_steps)\n",
        "\n",
        "  smoothed_rewards = {}\n",
        "  for agent, rs in rewards.items():\n",
        "    smoothed_rewards[agent] = np.array(rs)\n",
        "\n",
        "  PlotData = collections.namedtuple('PlotData',\n",
        "                                    ['title', 'data', 'opt_values', 'log_plot'])\n",
        "  total_regrets = dict([(k, np.cumsum(v, axis=1)) for k, v in regrets.items()])\n",
        "  plot_data = [\n",
        "      PlotData(title='Smoothed rewards', data=smoothed_rewards,\n",
        "               opt_values=opt_values, log_plot=False),\n",
        "      PlotData(title='Current Regret', data=regrets, opt_values=None,\n",
        "               log_plot=True),\n",
        "      PlotData(title='Total Regret', data=total_regrets, opt_values=None,\n",
        "               log_plot=False),\n",
        "  ]\n",
        "\n",
        "  plot(agents, plot_data, repetitions)\n",
        "\n",
        "def calculate_lims(data, log_plot=False):\n",
        "  y_min = np.min(data)\n",
        "  y_max = np.max(data)\n",
        "  diff = y_max - y_min\n",
        "  if log_plot:\n",
        "    y_min = 0.9*y_min\n",
        "    y_max = 1.1*y_max\n",
        "  else:\n",
        "    y_min = y_min - 0.05*diff\n",
        "    y_max = y_max + 0.05*diff\n",
        "  return y_min, y_max\n",
        "\n",
        "def argmax(array):\n",
        "  \"\"\"Returns the maximal element, breaking ties randomly.\"\"\"\n",
        "  return np.random.choice(np.flatnonzero(array == array.max()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzpb_dGVjT0O"
      },
      "source": [
        "# A) Agent implementations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBHsuFyapu5r"
      },
      "source": [
        "\n",
        "All agents should be in pure Python/NumPy.\n",
        "\n",
        "You cannot use any AutoDiff packages (Jax, TF, PyTorch, etc.)\n",
        "\n",
        "Each agent, should implement the following methods:\n",
        "\n",
        "**`step(self, previous_action, reward)`:**\n",
        "\n",
        "Should update the statistics by updating the value for the previous_action towards the observed reward.\n",
        "\n",
        "(Note: make sure this can handle the case that previous_action=None, in which case no statistics should be updated.)\n",
        "\n",
        "(Hint: you can split this into two steps: 1. update values, 2. get new action.  Make sure you update the values before selecting a new action.)\n",
        "\n",
        "**`reset(self)`:**\n",
        "\n",
        "Resets statistics (should be equivalent to constructing a new agent from scratch).\n",
        "\n",
        "Make sure that the initial values (after a reset) are all zero.\n",
        "\n",
        "**`__init__(self, name, number_of_arms, *args)`:**\n",
        "\n",
        "The `__init__` should take at least an argument `number_of_arms`, and (potentially) agent specific args."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwUUBXgQ2MCk"
      },
      "source": [
        "## Example agent\n",
        "\n",
        "The following code block contains an example random agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPYlY9M22JOI"
      },
      "outputs": [],
      "source": [
        "class Random(object):\n",
        "  \"\"\"A random agent.\n",
        "\n",
        "  This agent returns an action between 0 and 'number_of_arms', uniformly at\n",
        "  random. The 'previous_action' argument of 'step' is ignored.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, name, number_of_arms):\n",
        "    \"\"\"Initialise the agent.\n",
        "    \n",
        "    Sets the name to `random`, and stores the number of arms. (In multi-armed\n",
        "    bandits `arm` is just another word for `action`.)\n",
        "    \"\"\"\n",
        "    self._number_of_arms = number_of_arms\n",
        "    self.name = name\n",
        "\n",
        "  def step(self, unused_previous_action, unused_reward):\n",
        "    \"\"\"Returns a random action.\n",
        "    \n",
        "    The inputs are ignored, but this function still requires an action and a\n",
        "    reward, to have the same interface as other agents who may use these inputs\n",
        "    to learn.\n",
        "    \"\"\"\n",
        "    return np.random.randint(self._number_of_arms)\n",
        "\n",
        "  def reset(self):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDTyvlZsvSQq"
      },
      "source": [
        "\n",
        "## Q1 [2 pts]\n",
        "Implement a UCB agent.\n",
        "\n",
        "The `bonus_multiplier` is the parameter $c$ from the slides."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZM5NtZ3Q2X0F"
      },
      "outputs": [],
      "source": [
        "class UCB(object):\n",
        "  def __init__(self, name, number_of_arms, bonus_multiplier, t = 0, Na = [], Qa = []):\n",
        "    \n",
        "    self.number_of_arms = number_of_arms\n",
        "    self._bonus_multiplier = bonus_multiplier\n",
        "    self.name = name\n",
        "    self.t = 0 # step is needed as it's used in choosing what action to take\n",
        "    self.Na = np.zeros(self.number_of_arms) #  this array holds the information of how often each action has been taken\n",
        "    self.Qa = np.zeros(self.number_of_arms) # this array holds the value estimates for each action\n",
        "\n",
        "    #self.reset()\n",
        "    \n",
        "  def step(self, previous_action, reward):\n",
        "  # [my code] \n",
        "    UCBTerm = np.zeros(self.number_of_arms)\n",
        "    if previous_action == None and reward == None:\n",
        "      action = 0\n",
        "      self.t += 1\n",
        "      self.Na[action] += 1    \n",
        "      return action\n",
        "    else:\n",
        "      self.Qa[previous_action] += (1/self.Na[previous_action])*(reward-self.Qa[previous_action])\n",
        "      if self.t <= self.number_of_arms and self.t < self.number_of_arms-1:\n",
        "        action = previous_action + 1\n",
        "      elif self.t == self.number_of_arms-1:\n",
        "        action = self.t\n",
        "      else:\n",
        "        for a in range(self.number_of_arms):\n",
        "          UCBTerm[a] = UCBTerm[a] = self.Qa[a] + self._bonus_multiplier*np.sqrt(np.log(self.t)/self.Na[a]) \n",
        "        action = np.argmax(UCBTerm)\n",
        "      self.t += 1\n",
        "      self.Na[action] += 1\n",
        "      return action\n",
        "    \n",
        "  def reset(self):\n",
        "    self.number_of_arms = self.number_of_arms\n",
        "    self._bonus_multiplier = self._bonus_multiplier\n",
        "    self.name = self.name\n",
        "    self.t = 0 \n",
        "    self.Na = np.zeros(self.number_of_arms)  \n",
        "    self.Qa = np.zeros(self.number_of_arms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqJxDegZtqXN"
      },
      "source": [
        "## Q2 [1 pt]\n",
        "Implement an $\\epsilon$-greedy agent.\n",
        "\n",
        "This agent should be able to support time-changing $\\epsilon$ schedules.\n",
        "\n",
        "Thus, your agent should accept both constants and callables as constructor argument `epsilon`; callables are used to decay the $\\epsilon$ parameter over time, for instance according to a polynomial schedule: $\\epsilon_t = t^{-\\eta}$ with $\\eta \\in [0, 1]$).\n",
        "\n",
        "\n",
        "If multiple actions have the same value, ties should be broken randomly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_1pB2p7146i"
      },
      "outputs": [],
      "source": [
        "class EpsilonGreedy(object):\n",
        "  \"\"\"An epsilon-greedy agent.\n",
        "\n",
        "  This agent returns an action between 0 and 'number_of_arms'; with probability\n",
        "  `(1-epsilon)` it chooses the action with the highest estimated value, while\n",
        "  with probability `epsilon` it samples an action uniformly at random.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, name, number_of_arms, epsilon=0.1, Qa = [], Na = [], t= 0):\n",
        "    self.name = name\n",
        "    self.number_of_arms = number_of_arms\n",
        "    self.epsilon = epsilon\n",
        "    self.Qa = np.zeros(self.number_of_arms)\n",
        "    self.Na = np.zeros(self.number_of_arms)\n",
        "    self.t = t\n",
        "    #self.reset()\n",
        "\n",
        "  def step(self, previous_action, reward):\n",
        "    \"\"\"Update the learnt statistics and return an action.\n",
        "\n",
        "    A single call to step uses the provided reward to update the value of the\n",
        "    taken action (which is also provided as an input), and returns an action.\n",
        "    The action is either uniformly random (with probability epsilon), or greedy\n",
        "    (with probability 1 - epsilon).\n",
        "\n",
        "    If the input action is None (typically on the first call to step), then no\n",
        "    statistics are updated, but an action is still returned.\n",
        "    \"\"\"\n",
        "    if previous_action != None:\n",
        "      self.Na[previous_action] += 1\n",
        "      self.Qa[previous_action] += (1. / self.Na[previous_action]) * (reward - self.Qa[previous_action])\n",
        "    self.t += 1\n",
        "\n",
        "    epsilon = self.epsilon(self.t) if callable(self.epsilon) else self.epsilon\n",
        "    if np.random.rand() < epsilon:\n",
        "      action = np.random.randint(self.number_of_arms)\n",
        "    else:\n",
        "      action = argmax(self.Qa)\n",
        "    return action   \n",
        "   \n",
        "\n",
        "  def reset(self):\n",
        "    self.name = self.name\n",
        "    self.number_of_arms = self.number_of_arms\n",
        "    self.epsilon = self.epsilon\n",
        "    self.Qa = np.zeros(self.number_of_arms)\n",
        "    self.Na = np.zeros(self.number_of_arms)\n",
        "    self.t = 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enKI7uNjI1Ym"
      },
      "source": [
        "## Q3 [2 pts]\n",
        "Implement a REINFORCE agent.\n",
        "\n",
        "While `softmax` distributions are a common parametrization for policies over discrete action-spaces, they are not the only choice. In this exercise we ask you to implement REINFORCE with the `square-max` policy parameterization. With this parametrisation the probabilities depend on the action preferences $p(\\cdot)$ according to the expression:\n",
        "\n",
        "$$\\pi(a) = \\frac{p(a)^2}{\\sum_b p(b)^2}\\,.$$\n",
        "\n",
        "Implement a REINFORCE policy-gradient method for updating the preferences under this policy distribution. The action preferences are stored separately, so that for each action $a$ the preference $p(a)$ is a single value that you directly update.\n",
        "\n",
        "The agent should be able to use a baseline or not (as defined in the constructor). The `step_size` parameter $\\alpha$ used to update the policy must also be configurable in the constructor.\n",
        "\n",
        "The baseline should track the average reward so far, using the same `step_size` used to update the policy.\n",
        "\n",
        "The `step_size` and whether or not a baseline is used are defined in the constructor by feeding additional arguments in place of `...` below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqcC-OZq9bP7"
      },
      "outputs": [],
      "source": [
        "class REINFORCE(object):\n",
        "  def __init__(self, name, number_of_arms, step_size=0.1, baseline=False, preferences = [], t = 0, avg_reward = 0):\n",
        "    self.name = name\n",
        "    self.number_of_arms = number_of_arms\n",
        "    self.preferences = np.ones(number_of_arms)\n",
        "    self.baseline = baseline\n",
        "    self.t = t\n",
        "    self.avg_reward = avg_reward\n",
        "    self.step_size = step_size\n",
        "    #self.reset()\n",
        "\n",
        "  def step(self, previous_action, reward):\n",
        "    \n",
        "    if previous_action == None:\n",
        "        probabilities = self.preferences**2/(sum(self.preferences**2))\n",
        "        action = np.random.choice(np.arange(0, self.number_of_arms), p=probabilities)\n",
        "        self.t += 1\n",
        "        return action \n",
        "    else:\n",
        "        probabilities = self.preferences**2/(sum(self.preferences**2))\n",
        "        if self.baseline:\n",
        "            if self.step == 1:\n",
        "              self.avg_reward = reward\n",
        "            else:\n",
        "              self.avg_reward = (1/(self.t+1))*(reward - self.avg_reward)\n",
        "            for a in range(self.number_of_arms):\n",
        "              if a == previous_action:\n",
        "                self.preferences[a] += self.step_size*(reward - self.avg_reward)*(1-probabilities[a])\n",
        "              else:\n",
        "                self.preferences[a] -= self.step_size*(reward - self.avg_reward)*probabilities[a]\n",
        "            probabilities = self.preferences**2/(sum(self.preferences**2))\n",
        "            action = np.random.choice(np.arange(0, self.number_of_arms), p=probabilities)\n",
        "            self.t += 1\n",
        "            return action \n",
        "        else:\n",
        "            for a in range(self.number_of_arms):\n",
        "                  if a == previous_action:\n",
        "                    self.preferences[a] += self.step_size*(reward )*(1-probabilities[a])\n",
        "                  else:\n",
        "                    self.preferences[a] -= self.step_size*(reward )*probabilities[a]\n",
        "            probabilities = self.preferences**2/(sum(self.preferences**2))\n",
        "            action = np.random.choice(np.arange(0, self.number_of_arms), p=probabilities)\n",
        "            self.t += 1\n",
        "            return action \n",
        "\n",
        "  def reset(self):\n",
        "    self.name = self.name\n",
        "    self.number_of_arms = self.number_of_arms\n",
        "    self.preferences = np.ones(self.number_of_arms)\n",
        "    self.baseline = self.baseline\n",
        "    self.t = 0\n",
        "    self.step_size=self.step_size\n",
        "    self.avg_reward = 0\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jZsPzCmDxAh"
      },
      "source": [
        "# B) Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQkk8sMxE0N4"
      },
      "source": [
        "**Run the cell below to train the agents and generate the plots for the first experiment.**\n",
        "\n",
        "Trains the agents on a Bernoulli bandit problem with 5 arms,\n",
        "with a reward on success of 1, and a reward on failure of 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06P4AfZw1GSH"
      },
      "source": [
        "## Experiment 1: Bernoulli bandit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loe_YN7Tv8yY"
      },
      "outputs": [],
      "source": [
        "%%capture experiment1\n",
        "\n",
        "number_of_arms = 5\n",
        "number_of_steps = 1000\n",
        "\n",
        "agents = [\n",
        "    Random(\n",
        "        \"random\",\n",
        "        number_of_arms),\n",
        "    EpsilonGreedy(\n",
        "        r\"$\\epsilon$-greedy with $\\epsilon=0$\",\n",
        "        number_of_arms,\n",
        "        epsilon=0.),\n",
        "    EpsilonGreedy(\n",
        "        r\"$\\epsilon$-greedy with $\\epsilon=0.1$\",\n",
        "        number_of_arms,\n",
        "        epsilon=0.1),\n",
        "\n",
        "    EpsilonGreedy(\n",
        "        r\"$\\epsilon$-greedy with $\\epsilon_t=1/t$\",\n",
        "        number_of_arms,\n",
        "        epsilon=lambda t: 1./t),\n",
        "    EpsilonGreedy(\n",
        "        r\"$\\epsilon$-greedy with $\\epsilon_t=1/\\sqrt{t}$\",\n",
        "        number_of_arms,\n",
        "        epsilon=lambda t: 1./t**0.5),\n",
        "\n",
        "    UCB(\"UCB\",\n",
        "        number_of_arms,\n",
        "        bonus_multiplier=1/np.sqrt(2)),\n",
        "    REINFORCE(\n",
        "        r\"REINFORCE, $\\alpha=0.1$\",\n",
        "        number_of_arms,\n",
        "        step_size=0.1,\n",
        "        baseline=False),\n",
        "    REINFORCE(\n",
        "        r\"REINFORCE with baseline, $\\alpha=0.1$\",\n",
        "        number_of_arms,\n",
        "        step_size=0.1,\n",
        "        baseline=True),\n",
        "]\n",
        "\n",
        "train_agents(agents, number_of_arms, number_of_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnbpeszVp04-"
      },
      "outputs": [],
      "source": [
        "experiment1.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWyaMm-sjz9a"
      },
      "source": [
        "## Q4 [4 pts total]\n",
        "(Answer inline in the markdown below each question, **within this text cell**.)\n",
        "\n",
        "**[2 pts]**\n",
        "For each algorithm in the plots above, explain whether or not we should be expected it to be good in general, in terms of total regret.\n",
        "\n",
        "- Random algorithm: We expected for the random algorithm to perform very bad since it doesn't learn from the data like non-random (learning) algorithms and hence doesn't improve its choices and hence why its total regret is expected to be in the set of the worser algorithms.\n",
        "- $\\epsilon$ - greedy with $\\epsilon = 0$ algorithm: This algorithm doesn't pick any actions which don't yield the highest expected reward and hence it may be picking sub-optimal actions often, yielding bad results.\n",
        "- $\\epsilon$ - greedy with $\\epsilon = 0.1$ algorithm: This algorithm picks a random action 10% of the time, hence exploring other actions than the action with the highest expected reward with probability $\\frac{\\text{number of actions -1}}{\\text{number of actions}}$ (since it may pick the action with the highest expected reward as well). This algorithm is expected to perform well although it will have slow convergence.\n",
        "- $\\epsilon$ - greedy with $\\epsilon=\\frac{1}{t}$ and $\\epsilon$ - greedy with $\\epsilon=\\frac{1}{\\sqrt{t}}$ algorithms: These algorithms explore less as time passess. These algorithms are expected to perform well since for the first steps the algorithms get to explore the actions often, and once they explore all of the actions they gets a better estimate of the expected rewards and stick to those actions.\n",
        "- UCB algorithm: This algorithm was expected to perform well, because the algorithm explores efficiently i.e., it prioritizes actions which haven't been explored (even if those actions gave bad rewards) but it doesn't prioritize actions which gave low rewards consistently. This prioritization ensures that all of the actions are explored well enough so that the regret is minimal.\n",
        "- Gradient Algorithms with and without baseline: These algorithms were expected to perform well because they pick actions according to preference which are updated via gradient ascent. Since they don't act greedily, but choose their actions according to a probability distribution, it ensures that sufficient exploration happens and hence these algorithms are expected to perform well.\n",
        "\n",
        "\n",
        "**[2 pts]** Explain the relative ranking of the $\\epsilon$-greedy algorithms in this experiment.\n",
        "\n",
        "- $\\epsilon = 0$ vs $\\epsilon = \\frac{1}{t}$\n",
        "\n",
        "\n",
        " The worst performing algorithm is the one with $\\epsilon =0$ since the algorithm chooses the greedy action 100% of the time. This makes it hard for the agent to know the estimates of the other actions more accurately (because it doesn't pick those actions whilst acting greedily). If we compare it to the algorithms with $\\epsilon=\\frac{1}{t}$, it looks like as if these algorithms should behave (in terms of greediness) very similarly the more time increases. But the fact that $\\epsilon=\\frac{1}{t}$ explored aggressively during the first steps ensured that it had more information regarding the estimates of the other actions, and hence the better performance than $\\epsilon=0$.\n",
        "\n",
        "- $\\epsilon = \\frac{1}{t}$ vs $\\epsilon = 0.1$\n",
        "\n",
        "$\\epsilon = 0.1$ performed better than $\\epsilon = \\frac{1}{t}$ since $\\epsilon=\\frac{1}{t}$ became greedy (exploration probability $<0.1$ after the 10th step) very quickly and apparently with 5 arms, it is not sufficient to ensure enough exploration to beat $\\epsilon=0.1$'s performance. If more steps were given, $\\epsilon=\\frac{1}{t}$ would eventually give better results, as it would choose the greedy action (and maybe optimal) more often.\n",
        "\n",
        "- $\\epsilon = \\frac{1}{t}$ vs $\\epsilon = \\frac{1}{\\sqrt{t}}$\n",
        "\n",
        "The best performing algorithm, $\\epsilon = \\frac{1}{\\sqrt{t}}$ performed the best because it had more exploration during the initial steps, and hence it got better estimates early on. These esimates were better than the $\\epsilon=\\frac{1}{t}$'s algorithm because instead of exploring 10 steps before having its exploration probability cut down to $0.1$, it explored $100$ steps. And even after that it explored more and hence increased its confidence in its estimates. Eventually though, $\\epsilon=\\frac{1}{t}$ would give better results (on average) because it would choose the greedy action more often.\n",
        "\n",
        "- $\\epsilon = \\frac{1}{\\sqrt{t}} $ vs $\\epsilon = 0.1$\n",
        "\n",
        "These algorithms performed the best, however $\\epsilon = \\frac{1}{\\sqrt{t}} $  did better because it explored more during the inital steps and decreased its exploration once the estimates became more concrete. $\\epsilon=0.1$ was still exploring (with probability $0.1$) after getting good estimates of all the actions and this exploration became detrimental to its total regret."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YO5NDaPGDsp"
      },
      "source": [
        "## Experiment 2: reward = 0 on success, reward = -1 on failure.\n",
        "\n",
        "**Run the cell below to train the agents and generate the plots for the second experiment.**\n",
        "Reruns experiment 1 but on a different bernoulli bandit problem with 5 arms,\n",
        "with a reward on success of 0, and a reward on failure of -1.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "id": "7cvJf4WzmJXK"
      },
      "outputs": [],
      "source": [
        "%%capture experiment2\n",
        "number_of_arms = 5\n",
        "number_of_steps = 1000\n",
        "\n",
        "train_agents(agents, number_of_arms, number_of_steps,\n",
        "             success_reward=0., fail_reward=-1.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5RXnvnFLOGa"
      },
      "outputs": [],
      "source": [
        "experiment2.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GOe5RDsnj4J"
      },
      "source": [
        "## Q5 [2 pts]\n",
        "For each algorithm, note whether the performance changed significantly compared to the **experiment 1**, and explain why it did or did not.\n",
        "\n",
        "(Use at most two sentences per algorithm).\n",
        "\n",
        " - $\\epsilon = 0$: This algorithm's performance changed significantly because before when it was getting $0$ reward for staying at a sub-optimal action, the estimates didn't change much for this action relative to the others actions, but now that it is getting punished for staying at a sub-optimal action (and hence the estimate relative to the other actions decreases), it changes its actions faster, hence increasing the chance that it will pick the optimal action quicker.\n",
        "\n",
        " - $\\epsilon = 0.1$ The algorithm's performance has changed, but not significantly, because before it was exploring different actions (and getting good estimates of the non-greedy actions) but now that it is getting punished it changed its course of action quicker, yielding better results. \n",
        "\n",
        " - $\\epsilon = \\frac{1}{t}$ - The algorithm's performance increased slightly for the same reason as above.\n",
        "\n",
        " - UCB algorithm: The algorithm's performance increased slightly for the same reason as above.\n",
        "\n",
        "\n",
        " - Gradient algorithms: These algorithms' performances didn't change or increased very slightly. This is because before (with reward-failure = 1,0) it was choosing its actions by comparing an action to all other actions and now it it doing the same, meaning that the rewards themselves don't play an important role in choosing an action as opposed to the algorithms which base their choice on the average of the rewards (for a specific action) so far.\n",
        "\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRDmw4nyFI73"
      },
      "source": [
        "## Run the following cells"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Drtsr8Cc1OWl"
      },
      "source": [
        "## Experiment 3: Non-stationary bandit\n",
        " * Reward on `failure` changes from 0 to +2.\n",
        " * Reward on `success` remains at +1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4RseDt-MCkq"
      },
      "outputs": [],
      "source": [
        "%%capture experiment3\n",
        "\n",
        "number_of_arms = 3\n",
        "number_of_steps = 1984\n",
        "agents = [\n",
        "    Random(\n",
        "        \"random\",\n",
        "        number_of_arms),\n",
        "    EpsilonGreedy(\n",
        "        r\"$\\epsilon$-greedy with $\\epsilon=0.1$\",\n",
        "        number_of_arms,\n",
        "        epsilon=0.1),\n",
        "    EpsilonGreedy(\n",
        "        r\"$\\epsilon$-greedy with $\\epsilon_t=1/\\sqrt{t}$\",\n",
        "        number_of_arms,\n",
        "        epsilon=lambda t: 1./t**0.5),\n",
        "    UCB(\"UCB\",\n",
        "        number_of_arms,\n",
        "        bonus_multiplier=1/np.sqrt(2)),\n",
        "    REINFORCE(\n",
        "        r\"REINFORCE with baseline, $\\alpha=0.1$\",\n",
        "        number_of_arms,\n",
        "        step_size=0.1,\n",
        "        baseline=True),\n",
        "\n",
        "]\n",
        "\n",
        "roving_bandit_class = partial(NonStationaryBandit, change_is_good=True)\n",
        "train_agents(agents, number_of_arms, number_of_steps,\n",
        "             bandit_class=roving_bandit_class)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sErI1V9h1ScE"
      },
      "source": [
        "## Experiment 4: Non-stationary bandit\n",
        " * Reward on `failure` changes from 0 to +1.\n",
        " * Reward on `success` changes from +1 to 0.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yT_mZxCIAfg9"
      },
      "outputs": [],
      "source": [
        "%%capture experiment4\n",
        "\n",
        "number_of_arms = 3\n",
        "number_of_steps = 1984\n",
        "\n",
        "\n",
        "roving_bandit_class = partial(NonStationaryBandit, change_is_good=False)\n",
        "train_agents(agents, number_of_arms, number_of_steps,\n",
        "             bandit_class=roving_bandit_class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aWjHNwbEsDJ"
      },
      "outputs": [],
      "source": [
        "experiment3.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[2 pts]** In **experiment 3** explain the ranking in current regret after the change in rewards for all algorithms.\n",
        "\n",
        "> We can see that UCB starts performing the worst, followed by $\\epsilon = \\frac{1}{\\sqrt{t}}$, the gradient algorithm and finally the best performing algorithm becomes $\\epsilon = 0.1$. The reason UCB starts performing the worst is because it doesn't stop choosing the actions which were optimal until step 800 and since the reward for success doesn't change, the estimates for these actions don't decrease relative to the other actions, and hence these actions (which were good until step 800) keep on being picked. On the other hand we see that $\\epsilon = 0.1$ performs the best, because it explores more often than the other algorithms and adapts quickly. The gradient algorithm performs better than the others because there is a higher probability to choose a non-greedy action and hence for the agent to update its preferences quicker. Finally the $\\epsilon = \\frac{1}{\\sqrt{t}}$ algorithm has a similar (but not the same) problem with that of the UCB algorithm, in that it doesn't explore other actions because the exploration probability becomes very small due to $t$ being larger than $800$ and hence it continues picking the sub-optimal actions."
      ],
      "metadata": {
        "id": "tOh2SPQ5W5F7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s703_VCICCTL"
      },
      "outputs": [],
      "source": [
        "experiment4.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5x84zO7DK2_t"
      },
      "source": [
        "## Q6 [9 pts total]\n",
        "\n",
        "Observe the reward and regret curves above.  After 800 steps, the rewards change. In **experiment 3** `success` continues to yield a reward of +1, but `failure` changes from a reward of 0 to a reward of +2.  In **experiment 4**, `success` is now worth 0 and `failure` is worth +1.\n",
        "\n",
        "Below, we ask for explanations.  Answer each question briefly, using at most three sentences per question.\n",
        "\n",
        "**[2 pts]** In **experiment 3** explain the ranking in current regret after the change in rewards for all algorithms.\n",
        "\n",
        "> We can see that UCB starts performing the worst, followed by $\\epsilon = \\frac{1}{\\sqrt{t}}$, the gradient algorithm and finally the best performing algorithm becomes $\\epsilon = 0.1$. The reason UCB starts performing the worst is because it doesn't stop choosing the actions which were optimal until step 800 and since the reward for success doesn't change, the estimates for these actions don't decrease relative to the other actions, and hence these actions (which were good until step 800) keep on being picked. On the other hand we see that $\\epsilon = 0.1$ performs the best, because it explores more often than the other algorithms and adapts quickly. The gradient algorithm performs better than the others because there is a higher probability to choose a non-greedy action and hence for the agent to update its preferences quicker. Finally the $\\epsilon = \\frac{1}{\\sqrt{t}}$ algorithm has a similar (but not the same) problem with that of the UCB algorithm, in that it doesn't explore other actions because the exploration probability becomes very small due to $t$ being larger than $800$ and hence it continues picking the sub-optimal actions.\n",
        "\n",
        "**[2 pts]** In **experiment 4** explain the ranking in current regret after the change in rewards for all algorithms.\n",
        "\n",
        "> We can see that the ranking is the same (as in experiment 3) except that the UCB algorithm is now the best performing one, instead of the worst. The reason the other 3 have the same relative ranking is because the failure to success ratio didn't change. But the relative change of success  in this experiment (the decrease from 1 to 0) impacted the UCB algorithm significantly, because now since it was choosing those actions which were sub-optimal (after the 800'th step) and since the estimates decreased (due to the success reward decreasing), it considered other actions (because their estimates didn't decrease since they were not being picked) and quickly adapted, causing it to be the best performing algorithm, as observed in experiments 1 and 2.\n",
        "\n",
        "**[2 pts]** Explain how and why the current-regret curve for UCB in **experiment 3** differs from the curve in **experiment 4**.\n",
        "\n",
        "> Explained in the previous answers.\n",
        "\n",
        "**[3 pts]** In general, if rewards can be non-stationary, and we don't know the exact nature of the non-stationarity, how could we modify UCB to perform better?   Be specific and concise.\n",
        "\n",
        "> There are two algorithms that have been discussed which could help UCB be adapted to the non-staitonary case: the dicounted and sliding-window UCB. These algorithms explore more as time passes, compared to the original UCB. They do this by giving less importance to past rewards. The discounted UCB uses a discount factor $\\gamma\\in (0,1)$ to track reward estimates and this way it gives more weights to recent rewards in it estimates. It also gives more confidence to the most recent rewards and replaces the action count in its uncertainty term with a count that discounts past actions exponentially with $\\gamma$. Sliding-window UCB considers the last actions and doesn't consider all of them (and this is the difference, these algorithms are very similar).\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "poXJlbZWjwpq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "UCL_RL_assignment_2022,_part_I.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}